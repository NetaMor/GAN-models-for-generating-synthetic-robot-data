import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import os.path
from tqdm import tqdm

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
PATH_gan = r"C:\Users\netam\Documents\Python Scripts\gan_project"


class GeneratorB(nn.Module):
    def __init__(self):
        super(GeneratorB, self).__init__()

        self.fc1 = nn.Linear(6, 32)
        torch.nn.init.normal_(self.fc1.weight, mean=0.0, std=(2/6)**0.5)
        self.fc2 = nn.Linear(32, 16)
        torch.nn.init.normal_(self.fc2.weight, mean=0.0, std=(2/32)**0.5)
        self.fc3 = nn.Linear(16, 8)
        torch.nn.init.normal_(self.fc3.weight, mean=0.0, std=(2/16)**0.5)
        self.fc4 = nn.Linear(8, 10)
        torch.nn.init.xavier_uniform(self.fc4.weight)

    def forward(self, x, training=False):

        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        x = F.leaky_relu(self.fc3(x))
        x = torch.tanh(self.fc4(x))
        return x

class DiscriminatorB(nn.Module):
    def __init__(self):
        super(DiscriminatorB, self).__init__()

        self.fc1 = nn.Linear(10, 32)
        torch.nn.init.normal_(self.fc1.weight, mean=0.0, std=(2/10)**0.5)
        self.fc2 = nn.Linear(32, 16)
        torch.nn.init.normal_(self.fc2.weight, mean=0.0, std=(2/32)**0.5)
        self.fc3 = nn.Linear(16, 8)
        torch.nn.init.normal_(self.fc3.weight, mean=0.0, std=(2/16)**0.5)
        self.fc4 = nn.Linear(8, 6)
        torch.nn.init.normal_(self.fc4.weight, mean=0.0, std=(2/8)**0.5)
        self.fc5 = nn.Linear(6, 1)
        torch.nn.init.xavier_uniform(self.fc5.weight)

    def forward(self, x, training=False):

        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        x = F.leaky_relu(self.fc3(x))
        x = F.leaky_relu(self.fc4(x))
        x = torch.sigmoid(self.fc5(x))
        return x

class GanModelB(nn.Module):
    def __init__(self):
        super(GanModelB, self).__init__()
        self.generator = GeneratorB()
        self.discriminator = DiscriminatorB()

    def training_gan(self,train_tensor, trainloader, lr, beta1, b_size, epoch, epoch_dis, fixed_noise, opt_name ,scaler1 ,scaler2):
        try:
            num_epochs = epoch
            criterion = nn.BCELoss()
            optimizerD = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(beta1, 0.999))
            optimizerG = optim.Adam(self.generator.parameters(), lr=lr, betas=(beta1, 0.999))

            real_label = 1.0
            fake_label = 0.

            G_losses = []
            D_losses = []
            D_real_losses = []
            D_fake_losses = []

            for e in tqdm(range(1, num_epochs + 1)):
                print(f"'Epoch {e}' ")
                for j, (data, _) in enumerate(trainloader):
                    for i in range(epoch_dis):
                        data = data.to(device)
                        # Update discriminator network
                        ## Train with real batch
                        self.discriminator.train()
                        self.generator.eval()
                        self.discriminator.zero_grad()
                        real_features = data
                        real_features = real_features.to(device)
                        label = torch.full((int(real_features.size()[0]),), real_label, dtype=torch.float,
                                           device=device)
                        output = self.discriminator(real_features, training=True).view(-1)
                        error_D_real = criterion(output, label)
                        error_D_real.backward()
                        dis_real = output.mean().item()
                        ## Train with fake batch
                        noise = torch.from_numpy(np.random.normal(0, 0.2, [int(real_features.size()[0]), 6])).type(
                            torch.FloatTensor)
                        noise = noise.to(device)
                        fake = self.generator(noise, training=False)
                        label.fill_(fake_label).type(torch.FloatTensor)
                        output = self.discriminator(fake, training=True).view(-1)
                        error_D_fake = criterion(output, label)
                        error_D_fake.backward()
                        dis_fake1 = output.mean().item()
                        # Compute error of D
                        errD = error_D_real + error_D_fake
                        # Update D
                        optimizerD.step()

                    # Update generator network:
                    self.generator.train()
                    self.discriminator.eval()
                    self.generator.zero_grad()
                    label = torch.full((int(real_features.size()[0]),), real_label, dtype=torch.float, device=device)
                    noise = torch.from_numpy(np.random.normal(0, 0.2, [int(real_features.size()[0]), 6])).type(
                        torch.FloatTensor)
                    noise = noise.to(device)
                    fake = self.generator(noise, training=True)
                    output = self.discriminator(fake, training=False).view(-1)
                    errG = criterion(output, label)
                    errG.backward()
                    dis_fake2 = output.mean().item()
                    # Update G
                    optimizerG.step()

                    # Save Losses
                    G_losses.append(errG.item())
                    D_losses.append(errD.item())
                    D_real_losses.append(error_D_real.item())
                    D_fake_losses.append(error_D_fake.item())

                # Check losses
                if (e % 50 == 0) or (e == num_epochs):
                    print (f"[{e}/{num_epochs}][{i}/{b_size}]\tLoss_D: {errD.item()} \tLoss_G:{errG.item()} \tDis_for_real_data:{dis_real} \tDis_for_fake_data:{dis_fake1}/ {dis_fake2}")
        except:
            torch.save(self, os.path.join(PATH_gan, str(opt_name) + ".pth"))
        else:
            print("Training is complete")
            torch.save(self, os.path.join(PATH_gan, str(opt_name) + ".pth"))
        return G_losses, D_losses, D_real_losses, D_fake_losses
